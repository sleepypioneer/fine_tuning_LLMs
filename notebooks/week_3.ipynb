{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 study notes\n",
    "\n",
    "## Decoder Models\n",
    "\n",
    "- can be used for most tasks an encoder can be\n",
    "- decoder differ in their self attention mechanism, it uses masked self attention, words only see one context (the left or right rather than both) the other is hidden (masked) - unidirectional\n",
    "- good at text generation (causal language modeling)\n",
    "\n",
    "### Decoding Strategies\n",
    "\n",
    "- **Greedy Search**: Selects the most probably netx word, it may miss more nuanced options\n",
    "- **Bean Search**: Tracks multiple top candiates to find optimal outputs, might be repetitive\n",
    "- **Sampling**: Draws from a predicted distribution, adds some randomness and therefore can lead to incoherent text\n",
    "- **Temperature Scaling and Top-K Sampling**: filter unlikely candiates which can make sampling more cosistent\n",
    "- **Top-p (Nucleus) Sampling**: includes only top candiates which exceed a predefined probability threshold balancing diversity and quality\n",
    "\n",
    "### Types of Decoder Models\n",
    "\n",
    "- **Pretrained Models**: trained on vast amounts of data, can be used for few-shot tasks via prompt engineering\n",
    "- **Fine tuned Decoder**: leveraging transfer learning, fine tune a pretrained model's paramters on specific asks\n",
    "- **Instruction Tuning**: models are trained to follow natural language instrucions that describe the taska t hand i.e. Translate this text from German to French. [OpenAI blog post on instruction models](https://openai.com/research/instruction-following)\n",
    "\n",
    "\n",
    "## Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "- [Transformer Reinforcement Learning (TRL)]()\n",
    "- Human evaluators  assign scores or rating to assess the quality of the model's responses\n",
    "- High scores are rewarded, low scores are penalised, the LLM is retrained to maximize its expected reward\n",
    "- **Supervised Fine-tuning (SFT)**: train a model on a smaller dataset related to the target task\n",
    "- **Reward Modeling**: ratings or comparisons are used to give a signal of reward. Reinforcement learning (RL) is used to optimize the model in away where it preferences the higher ratings or scores\n",
    "- **[Proximal Policy Optimization (PPO)](https://arxiv.org/pdf/1707.06347.pdf)**: an reinforcement algorithm which optimizes the models policy (how is makes decisions) by balancing new behaviours while maintaining proximity to current policy (what it already knows). Leveraging clipping loss and value function approximations to ensure stable training and avoiding sudden major changes.\n",
    "\n",
    "## Direct Preference Optimization (DPO)\n",
    "\n",
    "Doesn't use a reard model (so removes this requirement). It frames the human feedback as a classification prioblem and directly optimizes parameters.\n",
    "\n",
    "[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)\n",
    "\n",
    "\n",
    "## Supervised Fine-tuning\n",
    "\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. prepare the dataset\n",
    "2. tokenization and formating\n",
    "3. Data collators (attention and causal masks for training are applied)\n",
    "4. Training arguments (hyper paramter fine tuning)\n",
    "5. Trainer setup\n",
    "6. Evaluation on test dataset\n",
    "\n",
    "## GPU Utilization\n",
    "\n",
    "### During training\n",
    "\n",
    "#### Factors\n",
    "\n",
    "1. Model parameters (more parameters more computation)\n",
    "2. Gradients (derivatives of the loss fucntion, more parameters means larger gradients)\n",
    "3. Intemdiate Tensors\n",
    "4. Gradient histories (if being tracked)\n",
    "5. Input Token size (larger number of tokens in each input the higher the GPU usage)\n",
    "6. Batch size (number of input examples process simultaneously) - need to balance this with training speed\n",
    "\n",
    "Therefore you can take these three steps to optimize for GPU usages:\n",
    "\n",
    "1. smaller batch\n",
    "2. efficient training methods\n",
    "3. upgrade the GPU for more memory\n",
    "\n",
    "## Parameter Efficient Fine Tuning (PEFT)\n",
    "\n",
    "Introduce methods introduce additional trainable parameters, such as adapters or prompts, which are tuned based on downstream data. These can be more performant than other fine tuning methods as they only alter a fraction of the parameters.\n",
    "\n",
    "## Low Rank Adaption (LoRA)\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "\n",
    "- The original pretrained weights remain untouched.\n",
    "- Introduces two smaller update matrices for each layer that capture the critical weight adjustments\n",
    "- reduced number of parameters\n",
    "- more efficient that a full weight train\n",
    "\n",
    "[Hugging Face guide to LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)\n",
    "\n",
    "## Prompt tuning\n",
    "\n",
    "- tunes a prompt to adapt model to new tasks\n",
    "\n",
    "[Hugging Face guide to prompt tuning](https://uplimit.com/course/fine-tuning-language-models/v2/module/parameter-efficient-fine-tuning#:~:text=checking%20out%20this-,Hugging%20Face%20guide,-.%20If%20you%27re%20interested)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
