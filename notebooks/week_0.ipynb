{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites Study Notes\n",
    "\n",
    "## NLP (Natural Language Processing)\n",
    "*understand, interpret, and generate human language*\n",
    "\n",
    "### Applications\n",
    "- Virtual Assistants\n",
    "- Sentiment Analysis\n",
    "- Summarization\n",
    "- Fraud Detection\n",
    "...\n",
    "\n",
    "### Why Now?\n",
    "- Transformer models can capture long-range dependencies in text\n",
    "- Large text datasets\n",
    "- Advances in computational power & Hardware like GPUs\n",
    "- Open Source libraries\n",
    "\n",
    "## Hugging Face Pipelines\n",
    "\n",
    "Hugging Face pipelines provide a quick and easy way to use models for inference. In as little as three lines of code, you can summarize text, translate languages, answer questions, generate text, fill in masked text, or perform a variety of other NLP tasks.\n",
    "\n",
    "1. Tokenization (break tokens into tokens)\n",
    "2. Encoding (turn into numerical representations)\n",
    "3. Post-processing (into human readable results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to microsoft/DialoGPT-medium and revision 8bada3b (https://huggingface.co/microsoft/DialoGPT-medium).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Conversation id: 9f1c109e-8353-4478-9a46-6259cddcc25d\n",
       " user: Going to the movies tonight - any suggestions?\n",
       " assistant: The Big Lebowski,\n",
       " Conversation id: 9c335953-80ae-4308-bb37-8d6f8a191a4b\n",
       " user: What's the last book you have read?\n",
       " assistant: The Last Question]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "converse = pipeline(\"conversational\")\n",
    "\n",
    "conversation_1 = Conversation(\"Going to the movies tonight - any suggestions?\")\n",
    "conversation_2 = Conversation(\"What's the last book you have read?\")\n",
    "converse([conversation_1, conversation_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.15k/1.15k [00:00<00:00, 3.00MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.63G/1.63G [00:16<00:00, 97.1MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 62.7kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.15MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.48MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.64MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5227586030960083,\n",
       "  0.45813971757888794,\n",
       "  0.014264645986258984,\n",
       "  0.0026850185822695494,\n",
       "  0.0021520694717764854]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"zero-shot-classification\",model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "pipe(\"I have a problem with my iphone that needs to be resolved asap!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 998/998 [00:00<00:00, 6.57MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.33G/1.33G [00:13<00:00, 96.0MB/s]\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 60.0/60.0 [00:00<00:00, 382kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 84.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9974554,\n",
       "  'index': 3,\n",
       "  'word': 'John',\n",
       "  'start': 5,\n",
       "  'end': 9},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9992238,\n",
       "  'index': 8,\n",
       "  'word': 'New',\n",
       "  'start': 24,\n",
       "  'end': 27},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99931407,\n",
       "  'index': 9,\n",
       "  'word': 'York',\n",
       "  'start': 28,\n",
       "  'end': 32},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99942446,\n",
       "  'index': 10,\n",
       "  'word': 'City',\n",
       "  'start': 33,\n",
       "  'end': 37}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(task=\"token-classification\")\n",
    "pipe(\"I am John and I live in New York City.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating models\n",
    "\n",
    "[Hugging Face evaluate](https://huggingface.co/docs/evaluate/main/en/index)\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Measure performance of a model on a given dataset. [Accuracy, Exact Match, Mean Intersection over Union (mIOU),...]\n",
    "\n",
    "### Comparisions\n",
    "\n",
    "Compare performance of two or more models on a test dataset, to see whether the models' predictions diverge or not.\n",
    "\n",
    "### Measurements\n",
    "\n",
    "Gain more insights on datasets and model predictions. Average length of the inputs for example can help when choosing input length for a Tokenizer.\n",
    "\n",
    "¦ NLP Task ¦ Evaluation Metric ¦\n",
    "¦ ------------------------ ¦ ----------------------------------- ¦\n",
    "¦ Text Classification      ¦ Accuracy, F1 Score, AUC-ROC         ¦\n",
    "¦ Token Classification     ¦ Precision, Recall, F1 Score         ¦\n",
    "¦ Question Answering       ¦ Exact Match , F1 Score              ¦\n",
    "¦ Named Entity Recognition ¦ Mean Intersection over Union (mIOU) ¦\n",
    "¦ Summarization            ¦ ROUGE                               ¦\n",
    "¦ Translation              ¦ BLEU                                ¦\n",
    "¦ Language Modeling        ¦ Perplexity                          ¦\n",
    "¦ Dialog Systems           ¦ BLEU                                ¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon\n",
    "\n",
    "### In-Context Learning\n",
    "Input demonstrates the desired task, and the model learns to perform the task by predicting the output without any parameter updates.\n",
    "\n",
    "**Zero-shot learning** completely relies on the model's pre-trained knowledge, with no demo input. Where are **One-shot learning** takes a single example in the input prompt to define the task. **Few-shot learning** takes a few examples in the input prompt to define the task, the variety of these examples helps the model infer the objective and response structure.\n",
    "\n",
    "### Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Retrieval augmented generation is a technique that combines LLMs with external knowledge sources. It uses first retrieves the most relevant passages from a knowledge source, and then uses the LLM to generate a response based on the retrieved passages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-llms-CO9pqyKk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
