{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 study notes\n",
    "\n",
    "## Transformers\n",
    "\n",
    "Encoder-Decoder architecture.\n",
    "**Ecoder** reads in a sequence of tokens and generates representations for each. It applies multi-headed self-attention to model relationships between all tokens simultaneously. The **Decoder** consumes these to generate output text. By calculating attention weights between tokens, transformers are able to focus on the most relevant parts of the text for an individual token.\n",
    "\n",
    "Paper on structured Attention Networks: https://arxiv.org/pdf/1702.00887.pdf\n",
    "\n",
    "### Encoder only Architecture\n",
    "\n",
    "- removing the decoder optimizes for natural language understanding tasks, where generation is not required.\n",
    "- excel at tasks like classification, NER, question answering and semantic search.\n",
    "- more compact but powerful\n",
    "- example: BERT\n",
    "\n",
    "### Decoder only Architecture\n",
    "\n",
    "- removing the encoder optimizes for natural language generation tasks, where the model is required to produce text.\n",
    "- excel at tasks like text completion, dialogue response generation, summarization, and translation.\n",
    "- struggle with tasks requiring deep language understanding.\n",
    "- example: GPT-3\n",
    "\n",
    "### Encoder-Decoder Architecture\n",
    "\n",
    "- can tackle a wide range of tasks, including those that require both natural language understanding and generation.\n",
    "- example: T5\n",
    "\n",
    "\n",
    "Further reading: https://sebastianraschka.com/blog/2023/llm-reading-list.html\n",
    "\n",
    "\n",
    "## Fine Tuning LLMs\n",
    "\n",
    "- LLMs are pre-trained on large datasets and then fine-tuned on a specific task.\n",
    "- pre-trained models are initially trained on a general domain dataset using self-supervised learning.\n",
    "- this provides a deep understanding of language structure and meaning.\n",
    "- however, if used as is, the model will not perform well on specific tasks, producing generic outputs, lacking coherence, or even being inappropriate.\n",
    "\n",
    "** Benefits**\n",
    "- cost effective (smaller model can be deployed, don't have to train from scratch)\n",
    "- privacy (don't have to share data with third parties or rely on black box solutions)\n",
    "\n",
    "### Framework for LLM fine-tuning\n",
    "\n",
    "#### Overview\n",
    "\n",
    "1. Research\n",
    "    - understand the problem space and associated risks\n",
    "    - translate into Specific NLP tasks\n",
    "    - research existing models and architectures\n",
    "    - establish strong baselines (measure critical aspects like accuracy, fairness, latency)\n",
    "    - set concrete goals that align with business objectives\n",
    "2. Development\n",
    "    - prepare, analyze, and document datasets (look for potential bias, document filtering steps, human review of labels)\n",
    "    - fine-tune base models (track experiments)\n",
    "    - evaluate fine-tuned models on unseen datasets on fairness and latency\n",
    "3. Production\n",
    "    - MLOps\n",
    "        - model optimization\n",
    "        - model versioning\n",
    "        - CI/CD\n",
    "        - model monitoring\n",
    "        - retraining\n",
    "    - pre-deployment testing\n",
    "        - check bias and fairness\n",
    "        - edge cases\n",
    "        - security (adversarial attacks)\n",
    "        - user feedback and A/B testing\n",
    "    - monitor post-deployment\n",
    "        - continuous monitoring\n",
    "        - user feedback & usage metrics\n",
    "        - retraining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
