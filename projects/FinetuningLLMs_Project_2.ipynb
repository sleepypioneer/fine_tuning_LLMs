{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sleepypioneer/fine_tuning_LLMs/blob/main/projects/Project_2_(Student_Version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4N2X9u-xPnh"
      },
      "source": [
        "# Project 2 Learning Goals\n",
        "\n",
        "1. **Fine-Tuning BERT**: Gain hands-on experience with fine-tuning a BERT model for Sentiment Analysis on financial data.\n",
        "2. **Tokenizer Usage**: Learn how to use a tokenizer for text-to-token mapping, padding, and truncation.\n",
        "3. **Training Setup**: Understand and utilize `TrainingArguments` and `Trainer` for model training.\n",
        "4. **Model Deployment**: Learn how to push models to the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCH7Tk6hHS9d"
      },
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q8VYxtmkHSgj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: transformers[torch]\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch] datasets evaluate --quiet\n",
        "!pip install einops --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YgPQ926NHcU6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Any, Union, Generator, Callable, Tuple\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import unittest\n",
        "from unittest.mock import Mock, patch\n",
        "\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXBpvdqEHOzW"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will be using the same dataset as Project 1, so let's just repeat some of that code here.  The Financial Phrasebook dataset is a relatively small dataset (<5000 examples) so we'll have a fairly aggressive train/test split (70/30).  Since Bert is pretrained, we don't need an enormous training set anyways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AtWkDmk4xND9"
      },
      "outputs": [],
      "source": [
        "# Load the dataset with the 'sentences_50agree' configuration\n",
        "phrasebank = load_dataset(\"financial_phrasebank\", \"sentences_50agree\")\n",
        "\n",
        "# Split the 'train' data into training and test sets\n",
        "phrasebank_split = phrasebank[\"train\"].train_test_split(test_size=0.3, shuffle=True)\n",
        "\n",
        "# Retrieve the string version of the three classes of sentiments\n",
        "sentiment_names = phrasebank[\"train\"].features[\"label\"].names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysb90eA2JtGi"
      },
      "source": [
        "## Retrieve Pretrained Model\n",
        "We will retrieve the pretrained [Bert model](https://huggingface.co/bert-base-uncased) from HuggingFace. This is an encoder model that can easily be fine tuned to a variety of tasks.  In this project, we'll be fine-tuning it for classification on the Financial Phrasebank dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VPsNVraxxOWN"
      },
      "outputs": [],
      "source": [
        "# Retrieve the model and tokenizer for 'bert-base-uncased'\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def model_init():\n",
        "    # Note that we are specifying the number of labels we want.\n",
        "    # This preconfigures the model with a softmax output layer over the appropriate number of classes.\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=len(sentiment_names), return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['negative', 'neutral', 'positive']\n"
          ]
        }
      ],
      "source": [
        "print(sentiment_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3tgjjO_NziL"
      },
      "source": [
        "## Tokenize Dataset\n",
        "In Project 1, we didn't tokenize the dataset because we only needed it ad hoc.  This time, we know that we'll be iterating over it a few times in training, so we'll tokenize the whole thing at first to save time later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GsIs3yZBOBCe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d4ea24aa455444a814e5f74e8a43836",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3392 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a3a1aaccc1348568815f9093e2fe3d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize_function(example: Dict[str, Union[str, int]]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Tokenizes a single example using a pre-trained tokenizer.\n",
        "\n",
        "    Args:\n",
        "        example: The example containing a sentence to tokenize.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing tokenized input_ids and attention_mask, both as PyTorch tensors.\n",
        "    \"\"\"\n",
        "    tokenized_example = tokenizer(\n",
        "        example[\"sentence\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokenized_example\n",
        "\n",
        "# Map the train and test sets to tokenized versions of that data using the tokenize_function()\n",
        "train_tokenized_datasets = phrasebank_split[\"train\"].map(tokenize_function, batched=True)\n",
        "test_tokenized_datasets = phrasebank_split[\"test\"].map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMw2b09DWDVW"
      },
      "source": [
        "## Collator\n",
        "Next we'll create a data collator, which will ensure that all our data is padded appropriately as it is loaded in batches to the model.  Passing the tokenizer to the data collator serves a specific purpose: it allows the collator to know how to handle padding and other sequence manipulations in a way that is consistent with how the original tokenization was done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mAIVk-A1xOYi"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qde2fPKPX0N6"
      },
      "source": [
        "## Setting Up Training Arguments\n",
        "\n",
        "Before we start the training process, we need to configure various training parameters. This is done using the `TrainingArguments` class from the Hugging Face Transformers library. Here's a breakdown of the parameters we are setting:\n",
        "\n",
        "- **output_dir**: This is the directory where the training outputs (like model checkpoints) will be saved. We set it to `\"phrasebank-sentiment-analysis\"`.\n",
        "\n",
        "- **evaluation_strategy**: This parameter defines how often the model should be evaluated during training. We set it to `\"steps\"`, meaning the model will be evaluated at regular step intervals.\n",
        "\n",
        "- **eval_steps**: This specifies the number of training steps between each evaluation. We set it to `100`.\n",
        "\n",
        "- **per_device_train_batch_size**: This is the batch size for each training step. A batch is a portion of the dataset used for training the model in a single step. We set it to `32`.\n",
        "\n",
        "- **logging_steps**: This defines how often training metrics should be logged. We set it to `100`, so metrics will be logged every 100 steps.\n",
        "\n",
        "- **num_train_epochs**: This is the number of times the training loop will iterate over the entire training dataset. We set it to `4`.\n",
        "\n",
        "By setting these parameters, we control various aspects of training, evaluation, and logging, making the training process more structured and easier to manage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lp66okolxOan"
      },
      "outputs": [],
      "source": [
        "########################\n",
        "# START YOUR CODE HERE #\n",
        "########################\n",
        "# REPLACE None WITH YOUR CODE\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"phrasebank-sentiment-analysis\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    per_device_train_batch_size=32,\n",
        "    logging_steps=100,\n",
        "    num_train_epochs=4)\n",
        "\n",
        "#########################\n",
        "# FINISH YOUR CODE HERE #\n",
        "#########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "KrK8DHe8qEw9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Test Your Code!\n",
        "class TestTrainingArguments(unittest.TestCase):\n",
        "\n",
        "    def test_training_args(self):\n",
        "        # Check each parameter\n",
        "        self.assertEqual(training_args.output_dir, \"phrasebank-sentiment-analysis\")\n",
        "        self.assertEqual(training_args.evaluation_strategy, \"steps\")\n",
        "        self.assertEqual(training_args.eval_steps, 100)\n",
        "        self.assertEqual(training_args.per_device_train_batch_size, 32)\n",
        "        self.assertEqual(training_args.logging_steps, 100)\n",
        "        self.assertEqual(training_args.num_train_epochs, 4)\n",
        "\n",
        "# Run the tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestTrainingArguments))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcTeEo7nYUJL"
      },
      "source": [
        "## Defining Custom Evaluation Metrics\n",
        "\n",
        "To evaluate the performance of our fine-tuned model, we define a function called `compute_metrics`. This function will compute the F1 score and accuracy for the model's predictions.\n",
        "\n",
        "Here's a breakdown of what the function does:\n",
        "\n",
        "- **f1_metric and accuracy_metric**: We load F1 and accuracy evaluation metrics using a hypothetical `evaluate.load` method. These metrics are widely used for classification tasks.\n",
        "\n",
        "- **logits, labels**: The function takes `eval_preds` as input, which is a tuple containing the logits (model outputs) and the true labels.\n",
        "\n",
        "- **predictions**: We use NumPy's `argmax` function to find the index (class label) with the maximum value for each logit vector. This converts the logits to class labels.\n",
        "\n",
        "- **f1_score**: We compute the F1 score using the loaded `f1_metric`. We set the average parameter to `\"macro\"` to calculate the metric independently for each class and then find the average.\n",
        "\n",
        "- **accuracy**: We compute the accuracy using the loaded `accuracy_metric`.\n",
        "\n",
        "The function then returns a dictionary containing these computed metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "agC_qxBKYUwg"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "    \"\"\"Computes F1 score and accuracy for model evaluation.\n",
        "\n",
        "    This function takes a tuple containing the predicted logits and true labels,\n",
        "    and computes the F1 score and accuracy. It uses pre-loaded evaluation metrics\n",
        "    for F1 and accuracy, assumed to be loaded via a hypothetical `evaluate.load` method.\n",
        "\n",
        "    Args:\n",
        "        eval_preds: A tuple containing two NumPy arrays.\n",
        "                    The first array contains the predicted logits.\n",
        "                    The second array contains the true labels.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the F1 score and accuracy as scalar values.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    # REPLACE None WITH YOUR CODE\n",
        "\n",
        "    # Load evaluation metrics\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    # Extract logits and labels from eval_preds\n",
        "    logits, labels = eval_preds\n",
        "\n",
        "    # Convert logits to class labels\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Compute F1 score and extract the scalar value\n",
        "    f1_result = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    f1_score = f1_result['f1'] if isinstance(f1_result, dict) else f1_result\n",
        "\n",
        "    # Compute accuracy and extract the scalar value\n",
        "    accuracy_result = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    accuracy_score = accuracy_result['accuracy'] if isinstance(accuracy_result, dict) else accuracy_result\n",
        "\n",
        "    #########################\n",
        "    # FINISH YOUR CODE HERE #\n",
        "    #########################\n",
        "\n",
        "    return {\"F1\": f1_score, \"Accuracy\": accuracy_score}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "RRY7kG9ZqdLt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 3.869s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Test Your Code!\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "class TestComputeMetrics(unittest.TestCase):\n",
        "\n",
        "    def test_compute_metrics(self):\n",
        "        # Create example data: 3 correct predictions, 3 incorrect predictions\n",
        "        true_labels = np.array([0, 1, 0, 1, 1, 0])\n",
        "        pred_logits = np.array([[0.7, 0.3], [0.4, 0.6], [0.6, 0.4], [0.35, 0.65], [0.8, 0.2], [0.4, 0.6]])\n",
        "\n",
        "        # Compute expected F1 and accuracy using sklearn\n",
        "        pred_labels = np.argmax(pred_logits, axis=-1)\n",
        "        expected_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
        "        expected_accuracy = accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "        # Compute metrics using the function to be tested\n",
        "        result = compute_metrics((pred_logits, true_labels))\n",
        "\n",
        "        # Validate the results\n",
        "        self.assertAlmostEqual(result[\"F1\"], expected_f1, places=5)\n",
        "        self.assertAlmostEqual(result[\"Accuracy\"], expected_accuracy, places=5)\n",
        "\n",
        "# Run the tests in the notebook\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestComputeMetrics))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_6PWrKUmUS2"
      },
      "source": [
        "### Initializing the Trainer\n",
        "\n",
        "In this section, we initialize the `Trainer` class provided by the Hugging Face Transformers library. The `Trainer` is responsible for managing the training and evaluation loops. Below is an explanation of each argument passed to the `Trainer`:\n",
        "\n",
        "- `model.to(torch_device)`: The pre-trained model fine-tuned for our specific task. It is moved to the device specified by `torch_device` (either CPU or GPU).\n",
        "  \n",
        "- `training_args`: This contains various training arguments like the output directory, evaluation strategy, batch size, etc., which are defined in a `TrainingArguments` object.\n",
        "  \n",
        "- `train_dataset=train_tokenized_datasets`: This is the tokenized version of our training dataset, which the `Trainer` will use during the training process.\n",
        "  \n",
        "- `eval_dataset=test_tokenized_datasets`: Similar to `train_dataset`, this is the tokenized version of our test dataset used during the evaluation steps.\n",
        "  \n",
        "- `data_collator=data_collator`: A data collator is responsible for batching together samples for training and evaluation. Here, we use a predefined data collator suitable for our task.\n",
        "  \n",
        "- `tokenizer=tokenizer`: The tokenizer is responsible for converting text into tokens that the model can understand. Although not strictly necessary for training, it is often useful for post-training tasks like inference.\n",
        "  \n",
        "- `compute_metrics=compute_metrics`: This function is used to compute evaluation metrics, like F1 score and accuracy, at the end of each evaluation loop.\n",
        "  \n",
        "By initializing the `Trainer` with these arguments, we set up a robust training and evaluation loop that takes care of most of the heavy lifting for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7OfDmcOvxOcs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "########################\n",
        "# START YOUR CODE HERE #\n",
        "########################\n",
        "# REPLACE None WITH YOUR CODE\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model_init(),\n",
        "  args=training_args,\n",
        "  train_dataset=train_tokenized_datasets,\n",
        "  eval_dataset=test_tokenized_datasets,\n",
        "  tokenizer=tokenizer,\n",
        "  data_collator=data_collator,\n",
        "  compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "#########################\n",
        "# FINISH YOUR CODE HERE #\n",
        "#########################\n",
        "\n",
        "# Note: There is not unit test for this function.\n",
        "# If the training loop in the next code cell works, then you've succeeded!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cRi4I1wo0rc"
      },
      "source": [
        "### Starting the Training Process\n",
        "\n",
        "The `trainer.train()` method is called to start the actual training of the model. This function initiates the training loop that iterates over the training dataset, updates the model parameters, and performs evaluations based on the configurations we set in `TrainingArguments` and `Trainer`.\n",
        "\n",
        "When this method is called, the following steps are executed:\n",
        "\n",
        "1. **Initialization**: The model and optimizer are initialized based on the configurations.\n",
        "  \n",
        "2. **Training Loop**: The model iterates over the training data in batches, performing forward and backward passes, and updating the model weights.\n",
        "  \n",
        "3. **Evaluation**: If specified in `TrainingArguments`, the model is evaluated on the test dataset at regular intervals. Metrics like F1 score and accuracy are computed using the `compute_metrics` function.\n",
        "  \n",
        "4. **Logging**: Training and evaluation statistics are logged, which can be viewed in real-time if a logging utility like TensorBoard is used.\n",
        "\n",
        "By calling this single method, the entire training, evaluation, and logging pipeline is executed, simplifying the process into a one-step operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "32kUQjWlxOey"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19419aa7a6204bf79d17bde7d017f56e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/424 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6137, 'learning_rate': 3.820754716981133e-05, 'epoch': 0.94}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac5de84c780b4e57af77e0208079f22f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/182 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5180354118347168, 'eval_F1': 0.7613839837778912, 'eval_Accuracy': 0.8060522696011004, 'eval_runtime': 30.9551, 'eval_samples_per_second': 46.971, 'eval_steps_per_second': 5.879, 'epoch': 0.94}\n",
            "{'loss': 0.297, 'learning_rate': 2.641509433962264e-05, 'epoch': 1.89}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad47aed565554364800b6f2d02872af5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/182 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.40177688002586365, 'eval_F1': 0.8201150844012707, 'eval_Accuracy': 0.842503438789546, 'eval_runtime': 11.1983, 'eval_samples_per_second': 129.841, 'eval_steps_per_second': 16.252, 'epoch': 1.89}\n",
            "{'loss': 0.1648, 'learning_rate': 1.4622641509433963e-05, 'epoch': 2.83}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e686480c47d942d8be4f464215cec11a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/182 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4640985131263733, 'eval_F1': 0.8327259565301834, 'eval_Accuracy': 0.8521320495185695, 'eval_runtime': 11.0453, 'eval_samples_per_second': 131.639, 'eval_steps_per_second': 16.478, 'epoch': 2.83}\n",
            "{'loss': 0.0736, 'learning_rate': 2.830188679245283e-06, 'epoch': 3.77}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d07a97be24574a1f8b90f55298b1cace",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/182 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5427023768424988, 'eval_F1': 0.8330949180475766, 'eval_Accuracy': 0.8493810178817056, 'eval_runtime': 10.9578, 'eval_samples_per_second': 132.691, 'eval_steps_per_second': 16.609, 'epoch': 3.77}\n",
            "{'train_runtime': 501.1459, 'train_samples_per_second': 27.074, 'train_steps_per_second': 0.846, 'train_loss': 0.27373447733105355, 'epoch': 4.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=424, training_loss=0.27373447733105355, metrics={'train_runtime': 501.1459, 'train_samples_per_second': 27.074, 'train_steps_per_second': 0.846, 'train_loss': 0.27373447733105355, 'epoch': 4.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgUgyH84pGJL"
      },
      "source": [
        "### Authentication and Model Upload to Hugging Face Hub\n",
        "\n",
        "#### Authentication\n",
        "The `notebook_login()` function from the `huggingface_hub` library is used to authenticate your notebook with your Hugging Face account. This step is essential for pushing models to the Hugging Face Model Hub. A pop-up will appear that will ask for your Hugging Face credentials.\n",
        "\n",
        "#### Pushing Model to the Hub\n",
        "After successful authentication, we call `trainer.push_to_hub()` to upload the trained model to the Hugging Face Model Hub.\n",
        "\n",
        "Here's what happens when you execute this code:\n",
        "\n",
        "1. **Authentication**: The `notebook_login()` function prompts you to log in to your Hugging Face account, allowing you secure access to push models to the hub.\n",
        "\n",
        "2. **Model Upload**: The `trainer.push_to_hub()` method uploads all model files (model weights, configuration, etc.) to your Hugging Face account. The model will be publicly available, and others can download it using its identifier.\n",
        "\n",
        "By running these commands, you not only preserve your model but also make it accessible to the wider community for various NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "if len(token) <= 0:\n",
        "    raise ValueError(\"You need to set your Hugging Face token to upload your model to the Hub.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.args.hub_model_id = \"finetuning-llms-project-2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /Users/jessica-g/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2fefd79eebc4fbda45134f9e2ba7d4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2f415177e274445b7f0df4ddfea3c16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/4.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef33c3a1f3804377a2e9a7a752f164d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find your new model here:  https://huggingface.co/jessica-ecosia/finetuning-llms-project-2/tree/main/\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=token)\n",
        "\n",
        "trainer.args.hub_model_id = \"finetuning-llms-project-2\"\n",
        "\n",
        "trainer.init_hf_repo()\n",
        "\n",
        "\n",
        "# Push to the hub\n",
        "model_url = trainer.push_to_hub()\n",
        "print(f'Find your new model here:  {model_url}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlQILuCTsgpP"
      },
      "source": [
        "# Project 2 Wrap-Up\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this project, we successfully achieved the following learning goals:\n",
        "\n",
        "### Fine-Tuning BERT\n",
        "We downloaded a pre-trained BERT model and fine-tuned it for the task of Sentiment Analysis, specifically focusing on financial data. This gave us hands-on experience with adapting a general-purpose language model to a specialized task.\n",
        "\n",
        "### Tokenizer Usage\n",
        "We learned how to use a tokenizer for essential text processing steps such as text-to-token mapping, padding, and truncation. This is crucial for preparing textual data for model training.\n",
        "\n",
        "### Training Setup\n",
        "We utilized the `TrainingArguments` and `Trainer` classes from the Hugging Face Transformers library. This encapsulates best practices for training transformer models and provided a streamlined way to set up and execute the training process.\n",
        "\n",
        "### Model Deployment\n",
        "Finally, we pushed our fine-tuned model to the Hugging Face Hub. This allows for easy sharing of the model and offers a platform for community evaluation and usage.\n",
        "\n",
        "## Optional Steps for Future Exploration\n",
        "- **Evaluation Metrics**: Dive deeper into the evaluation metrics, possibly comparing them with benchmarks or other models.\n",
        "- **Model Interpretability**: Investigate why the model makes specific predictions to understand it better.\n",
        "- **Hyperparameter Tuning**: Experiment with different hyperparameters to potentially improve model performance.\n",
        "- **Version Control**: Learn to manage different versions of the model on the Hub.\n",
        "- **Real-World Testing**: Demonstrate how to use the deployed model for sentiment analysis on new financial data.\n",
        "- **Documentation**: Add detailed documentation to enhance the project's understandability and reusability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4jb5i0_hsmsF"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e479cc0a03ea449f8971a13b7cfdd60c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/182 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_loss: 0.5333\n",
            "eval_F1: 0.8384\n",
            "eval_Accuracy: 0.8556\n",
            "eval_runtime: 11.5913\n",
            "eval_samples_per_second: 125.4390\n",
            "eval_steps_per_second: 15.7010\n",
            "epoch: 4.0000\n"
          ]
        }
      ],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Interpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text = train_tokenized_datasets[0][\"sentence\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]: 0.0000\n",
            "the: -0.4624\n",
            "third: -0.3298\n",
            "applicant: -0.0203\n",
            ",: 0.0543\n",
            "fort: -0.4338\n",
            "##um: -0.1755\n",
            ",: 0.1876\n",
            "was: -0.3589\n",
            "dropped: 0.1164\n",
            ".: 0.5269\n",
            "[SEP]: 0.0000\n"
          ]
        }
      ],
      "source": [
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "\n",
        "model = model_init()\n",
        "\n",
        "explainer = SequenceClassificationExplainer(model, tokenizer)\n",
        "word_attributions = explainer(test_text)\n",
        "\n",
        "for word, attribution in word_attributions:\n",
        "    print(f\"{word}: {attribution:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning:\n",
        "\n",
        "You can use libraries like Optuna or Ray Tune for hyperparameter optimization with HuggingFace Trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "To use hyperparameter search, you need to pass your model through a model_init function.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/fg/sdhdjs9d49z5tc2s3p0lkxb80000gn/T/ipykernel_28409/2137994960.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trials = trainer.hyperparameter_search(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ray\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;31m# number of trials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m~/Documents/sleepypioneer/fine_tuning_LLMs/.env/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   2662\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp_search_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2664\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2665\u001b[0m                 \u001b[0;34m\"To use hyperparameter search, you need to pass your model through a model_init function.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: To use hyperparameter search, you need to pass your model through a model_init function."
          ]
        }
      ],
      "source": [
        "trials = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\", \n",
        "    backend=\"ray\", \n",
        "    n_trials=10 # number of trials\n",
        ")\n",
        "\n",
        "trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ray.tune.suggest'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/fg/sdhdjs9d49z5tc2s3p0lkxb80000gn/T/ipykernel_28409/1951043899.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHyperOptSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mASHAScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m best_trial = trainer.hyperparameter_search(\n\u001b[1;32m      5\u001b[0m     \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.tune.suggest'"
          ]
        }
      ],
      "source": [
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    # Choose among many libraries:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n",
        "    search_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),\n",
        "    # Choose among schedulers:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\n",
        "    scheduler=ASHAScheduler(metric=\"objective\", mode=\"max\")\n",
        ")\n",
        "\n",
        "best_trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version Control\n",
        "\n",
        "Each repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences. Version control allows revisions, a method for pinning a specific version of a model with a commit hash, tag or branch.\n",
        "\n",
        "As a result, you can load a specific model version with the revision parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08ff8f7d5d274f088a5567681b26dbdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)3dd7c3a2/config.json:   0%|          | 0.00/881 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46b9fa89d2b6493098af3bfcb45ff50c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_init()model_2 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"jessica-ecosia/finetuning-llms-project-2\", revision=\"b83fc52bf6959921d21975f3be7ce7d03dd7c3a2\"  # commit hash\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'BertForSequenceClassification' object has no attribute 'predict'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/fg/sdhdjs9d49z5tc2s3p0lkxb80000gn/T/ipykernel_28409/3449763004.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The financial outlook for the next quarter looks promising.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/Documents/sleepypioneer/fine_tuning_LLMs/.env/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "\n",
        "#.predict(\"The financial outlook for the next quarter looks promising.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"your-org-or-username/your-model-name\", tokenizer=\"your-tokenizer-path\")\n",
        "\n",
        "result = sentiment_pipeline(\"The financial outlook for the next quarter looks promising.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation:\n",
        "\n",
        "We can create a model card and push it to the Hub with our model so that others can understand how to use it. The model card is defined in the README.md file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.create_model_card(\n",
        "    model_name=\"finetuned-bert-sentiment-analysis\",\n",
        "    tags=[\"bert\", \"finance\", \"sentiment-analysis\"],\n",
        "    language=\"en\",\n",
        "    dataset=[\"financial_phrasebank\"],\n",
        "    license=\"MIT\",\n",
        "    finetuned_from=\"bert-base-uncased\",\n",
        "    dataset_tags=[\"finance\", \"sentiment-analysis\"],\n",
        "    dataset_args=[\"sentences_50agree\"],\n",
        ")\n",
        "\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
