{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sleepypioneer/fine_tuning_LLMs/blob/week_three/projects/FinetuningLLMs_Project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 3:  LORA Fine Tuning\n",
        "\n",
        "In this project, you will use parameter efficient fine tuning (PEFT) low rank adaptation (LORA) to fine-tune the phi-1.5 model for summarizing scientific papers. For this assignment, we will use `microsoft/phi-1.5` [model](https://huggingface.co/microsoft/phi-1_5) that works on free T4 GPUs with 16GB provided by Google Colab. If you have access to bigger GPUs like A100 with 40GB memory, I encourage you to experiment with bigger and latest LLMs like [Lllama-2](https://huggingface.co/models?search=llama2) or [Mistral](https://huggingface.co/mistralai)\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [1 - Data Handling](#1)\n",
        "  - [1.1 - Downloading the Data](#1.1)\n",
        "  - [1.2 - Exploring the Data](#1.2)\n",
        "\n",
        "- [2 - Data Preprocessing](#2)\n",
        "  - [2.1 - Restructuring and Tokenizing](#2.1)\n",
        "  - [2.2 - Decoding Example](#2.2)\n",
        "\n",
        "- [3 - Configuring PEFT LORA](#3)\n",
        "  - [3.1 - Downloading and Converting Phi-1.5](#3.1)\n",
        "  - [3.2 - Trainable Parameters](#3.2)\n",
        "\n",
        "- [4 - Training Configuration](#4)\n",
        "\n",
        "- [5 - Model Training](#5)\n",
        "\n",
        "- [6 - Evaluation](#6)\n",
        "\n",
        "- [7 - Real-World Application](#7)\n",
        "\n",
        "- [8 - Conclusion](#8)\n",
        "\n",
        "This project is truly the bleeding edge of generative AI- we are so excited to see where you take this technology.  If you get stuck, make sure to reach out to the course team and your peers for support!\n",
        "\n",
        "Let's get started."
      ],
      "metadata": {
        "id": "SvYvn0PYdsuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] datasets peft einops trl --quiet"
      ],
      "metadata": {
        "id": "7LOzjAo7tUMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, pipeline\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from trl import SFTTrainer\n",
        "import os\n",
        "import unittest\n",
        "from unittest.mock import patch\n",
        "import torch"
      ],
      "metadata": {
        "id": "sdYaBwJ_WGt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "# Data Handling\n",
        "\n",
        "Our goal is to fine tune a pretrained model on a dataset gathered by the [Allen Institute for AI](https://allenai.org/).  This dataset, known as [SciTLDR](https://huggingface.co/datasets/allenai/scitldr) contains extreme summaries of scientific content.  Here's a quick description from the dataset card:\n",
        "\n",
        "**SciTLDR: Extreme Summarization of Scientific Documents**\n",
        "\n",
        "SciTLDR is a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden.\n",
        "\n",
        "<a name='1.1'></a>\n",
        "## Downloading the Data\n",
        "Let's get started by pulling the dataset from the HuggingFace Hub."
      ],
      "metadata": {
        "id": "ZllPCuXMfns9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"allenai/scitldr\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "KBEURmuYv0Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1.2'></a>\n",
        "## Exploring the Data\n",
        "Let's take a look at an example from the data.  The `source` and `target` columns are most important to our work here, although the others also contain interesting information.  Don't hesitate to check them out!\n",
        "\n",
        "This dataset has a unique structure:\n",
        "* The source content for each sample is a list of sentences.  These sentences need to be concatenated to construct the full content.\n",
        "* THe target content, which contains the TLDR summary, is a list of strings.  Each of those strings contains a TLDR summary.  In this exercise, we are only using the first TLDR string in the dataset. This is the \"expert\" TLDR, according to the dataset description.\n",
        "\n",
        "Check out the cell below to see these concepts in practice."
      ],
      "metadata": {
        "id": "XTPVqiXihXNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rich import print\n",
        "sample_idx = 0\n",
        "\n",
        "# Concatenate all the source content sentences\n",
        "print(f\"Source content:  {' '.join(dataset['train'][sample_idx]['source'])}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Retrieve the first target TLDR.\n",
        "print(f\"Target TLDR:  {dataset['train'][sample_idx]['target'][0]}\")"
      ],
      "metadata": {
        "id": "VJsofQL-hwcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "# Load pre-trained model and tokenizer\n",
        "\n",
        "We'll be using the tokenizer that was trained in concert with the Phi-1.5 model.  This tokenizer does not come with a `PAD` token, so we will reuse its `EOS` token.  Let's download the tokenizer in the next cell."
      ],
      "metadata": {
        "id": "XvTX__d1qlKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='microsoft/phi-1_5'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "Ho49jh9cte-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained model performance\n",
        "\n",
        "Let's look at how the pre-trained model performs for summarization task. Let's structure the prompt as a zero-shot instruction prompt:\n",
        "\n",
        "```text\n",
        "Summarize the following academic content.\n",
        "\n",
        "[source scientific content]\n",
        "\n",
        "Summary:\n",
        "```"
      ],
      "metadata": {
        "id": "GXfgEvQVgHS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt template\n",
        "\n",
        "Let's create a prompt template that uses the source field in the dataset and converts source into an input to the model that includes the instruction."
      ],
      "metadata": {
        "id": "8A5v7eNhiepw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Summarize the following academic content.\\n\n",
        "{source}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "sample = dataset[\"train\"][0]\n",
        "prompt = prompt_template.format(source=\" \".join(sample[\"source\"]))\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "PNepqbamiQMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text-generation Pipeline\n",
        "\n",
        "Let's create text generation pipeline using the phi_1.5 model, pass the above prompt as input and evaluate the response of the model."
      ],
      "metadata": {
        "id": "C57Q2qDTjjDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\n",
        "response = pipe(prompt, do_sample=True, max_new_tokens=50, temperature=0.7)\n",
        "print(response[0][\"generated_text\"])\n",
        "print(\"-----------\")\n",
        "target = \" \".join(sample[\"target\"])\n",
        "print(\"TLDR target: %s\"%target)"
      ],
      "metadata": {
        "id": "SUa3-cPCjiiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restructuring a new column `text`\n",
        "Let's create a new column named `text` that appends the instruction, text source and the summary that we can use for fine-tuning."
      ],
      "metadata": {
        "id": "vx7SeJEolEzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_with_response = \"\"\"Summarize the following academic content.\\n\n",
        "{source}\n",
        "\n",
        "Summary: {target} {eos_token}\"\"\"\n",
        "\n",
        "sample = dataset[\"train\"][0]\n",
        "text = prompt_template_with_response.format(source=\" \".join(sample[\"source\"]), target=\" \".join(sample[\"target\"]), eos_token = tokenizer.eos_token)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "nNkq7qd5mOSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_text(batch):\n",
        "    \"\"\"Constructs a text from sources and targets with special prompt format for a summarization task.\n",
        "\n",
        "    Constructs a prompt by prepending a start prompt and appending an end prompt\n",
        "    to each source entry in the batch.\n",
        "\n",
        "    Args:\n",
        "        batch: A batch of source and target texts to tokenize.\n",
        "            The 'source' key should map to a list of source texts (each being a list of strings),\n",
        "            and the 'target' key should map to a list of target texts (each being a list of strings).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a new key named text\n",
        "    \"\"\"\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    # Replace None with your code\n",
        "\n",
        "    # Concatenate the source texts into single strings for each sample\n",
        "    sources = [None for src in batch['source']]\n",
        "\n",
        "    # Extract the first target text for each sample\n",
        "    targets = [None for target in batch[\"target\"]]\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    for source, target in zip(sources,targets):\n",
        "      text = prompt_template_with_response.format(source=None, target=None, eos_token = tokenizer.eos_token)\n",
        "      texts.append(text)\n",
        "\n",
        "    batch[\"text\"] = texts\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "krOaMKuym-AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a text column\n",
        "\n",
        "Use dataset `map` function to generate a `text` column, remove all the remaining columns and verify one of the samples."
      ],
      "metadata": {
        "id": "QxavbMOqpWQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_with_text = dataset.map(construct_text, batched=True, remove_columns=dataset['train'].column_names)\n",
        "\n",
        "dataset_with_text[\"train\"][0]"
      ],
      "metadata": {
        "id": "_3sc0d_5ySXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print the model to find attention blocks for LoRA"
      ],
      "metadata": {
        "id": "qtvUy2bAVN2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the model here"
      ],
      "metadata": {
        "id": "-8FS9s74Plm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4'></a>\n",
        "# Configuring PEFT LORA\n",
        "<a name='4.1'></a>\n",
        "## Downloading and Converting Phi-1.5\n",
        "Now that the data is prepared for the training process, let's download and prepare the model!  This is much more straightforward.  We'll simply configure the LORA using a `LoraConfig` class with the following parameters:\n",
        "* `r = 8` This is the rank of the trainable LORA matrix.\n",
        "* `lora_alpha = 16` As a rule of thumb, set LoRA Alpha to be twice the rank.\n",
        "* `target_modules = ['Wqkv']` This is the layer of the transformer to apply LoRA for phi-1.5. You can find the modules by printing the model to stdout. This layer will be different for different models. You can provide a list of modules to apply LoRA.\n",
        "* `lora_dropout = 0.05` This is the dropout probability of the LoRA layers.\n",
        "* `bias = 'none'` This deactivates bias parameter training.\n",
        "* `task_type = TaskType.CAUSAL_LM` This informs the lora configuration that the model is a causal language model.\n",
        "\n",
        "One the LORA is configured, we'll apply it to the model using the `get_peft_model()` function."
      ],
      "metadata": {
        "id": "eL7B1tT5zuMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# START YOUR CODE HERE #\n",
        "########################\n",
        "# Replace None with your code\n",
        "\n",
        "def create_lora_config():\n",
        "    lora_config = LoraConfig(\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None\n",
        "    )\n",
        "\n",
        "    return lora_config\n",
        "\n",
        "######################\n",
        "# END YOUR CODE HERE #\n",
        "######################"
      ],
      "metadata": {
        "id": "KCtZG-tm5p46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test your code!\n",
        "class TestLoraConfig(unittest.TestCase):\n",
        "    \"\"\"Unit tests for the LoraConfig class.\"\"\"\n",
        "\n",
        "    def test_lora_config_initialization(self):\n",
        "        \"\"\"Tests the initialization of LoraConfig with correct arguments.\"\"\"\n",
        "        lora_config = create_lora_config()\n",
        "\n",
        "        self.assertEqual(lora_config.r, 8)\n",
        "        self.assertEqual(lora_config.lora_alpha, 16)\n",
        "        self.assertEqual(lora_config.target_modules, {'Wqkv'})\n",
        "        self.assertEqual(lora_config.lora_dropout, 0.05)\n",
        "        self.assertEqual(lora_config.bias, \"none\")\n",
        "        self.assertEqual(lora_config.task_type, TaskType.CAUSAL_LM)\n",
        "\n",
        "# Run the tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestLoraConfig))"
      ],
      "metadata": {
        "id": "bXQCnwxk1pB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = create_lora_config()\n",
        "\n",
        "# Apply the LORA configuration to get a new model for PEFT\n",
        "peft_model = get_peft_model(model,\n",
        "                            lora_config)"
      ],
      "metadata": {
        "id": "jImO4oDU1pK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4.2'></a>\n",
        "## Trainable Parameters\n",
        "Let's now compare the number of parameters in the model with the number of trainable parameters."
      ],
      "metadata": {
        "id": "ysZCOAKr44Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    \"\"\"Prints the number of trainable parameters in the model.\n",
        "\n",
        "    This function traverses all the parameters of a given PyTorch model to\n",
        "    count the total number of parameters as well as the number of trainable\n",
        "    (i.e., requires gradient) parameters.\n",
        "\n",
        "    Args:\n",
        "        model: A PyTorch model whose parameters you want to count.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize counters for trainable and total parameters\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "\n",
        "    # Loop through all named parameters in the model\n",
        "    for _, param in model.named_parameters():\n",
        "        # Update the total number of parameters\n",
        "        all_model_params += param.numel()\n",
        "\n",
        "        # Check if the parameter requires gradient and update the trainable parameter counter\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "\n",
        "    # Calculate and print the number and percentage of trainable parameters\n",
        "    print(f\"Trainable model parameters: {trainable_model_params}\")\n",
        "    print(f\"All model parameters: {all_model_params}\")\n",
        "    print(f\"Percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\")\n",
        "\n",
        "print_number_of_trainable_model_parameters(peft_model)"
      ],
      "metadata": {
        "id": "9Q0F-coj5GG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='5'></a>\n",
        "# Training Configuration\n",
        "ALright, the data is ready ✅ and the LORA model is ready ✅, so our final step is to configure the training. Let's start with a baseline fo the following training arguments:\n",
        "* `learning_rate = 1e-3`: Specifies the learning rate for the optimizer.\n",
        "  \n",
        "* `num_train_epochs=1`: Indicates the number of times the entire training dataset is used to update the model weights. A value of 1 means each sample is used once to update the weights.\n",
        "\n",
        "* `logging_steps=50`: Specifies that logs for training metrics will be printed every 50 steps. Useful for monitoring the training process.\n",
        "\n",
        "* `evaluation_strategy=\"steps\"`: Specifies that the evaluation will be done based on the number of steps, as opposed to epochs. This is in line with `eval_steps`.\n",
        "\n",
        "* `eval_steps=200`: Indicates that the model will be evaluated every 200 training steps. This is only applicable if `evaluation_strategy` is set to `\"steps\"`.\n",
        "\n",
        "* `per_device_train_batch_size=1`: Defines the batch size for training on each device (e.g., GPU). A batch size of 1 means each training step updates the model based on a single sample.\n",
        "\n",
        "* `per_device_eval_batch_size=1`: Sets the batch size for evaluation on each device. Similar to the training batch size, a value of 1 means the model will be evaluated one sample at a time.\n"
      ],
      "metadata": {
        "id": "UT-wv8UB5NU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"uplimit-project-3-phi-1.5\"\n",
        "\n",
        "########################\n",
        "# START YOUR CODE HERE #\n",
        "########################\n",
        "# Replace None with your code\n",
        "\n",
        "def create_peft_training_args():\n",
        "    peft_training_args = TrainingArguments(\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "        None,\n",
        "    )\n",
        "\n",
        "    return peft_training_args\n",
        "\n",
        "######################\n",
        "# END YOUR CODE HERE #\n",
        "######################"
      ],
      "metadata": {
        "id": "RoAQRb_1t_oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your code\n",
        "\n",
        "class TestTrainingArguments(unittest.TestCase):\n",
        "    \"\"\"Unit tests for the TrainingArguments class.\"\"\"\n",
        "\n",
        "    def test_training_arguments_initialization(self):\n",
        "        \"\"\"Tests the initialization of TrainingArguments with correct arguments.\"\"\"\n",
        "        peft_training_args = create_peft_training_args()\n",
        "\n",
        "        self.assertEqual(peft_training_args.learning_rate, 1e-3)\n",
        "        self.assertEqual(peft_training_args.num_train_epochs, 1)\n",
        "        self.assertEqual(peft_training_args.logging_steps, 50)\n",
        "        self.assertEqual(peft_training_args.evaluation_strategy, \"steps\")\n",
        "        self.assertEqual(peft_training_args.eval_steps, 200)\n",
        "        self.assertEqual(peft_training_args.per_device_train_batch_size, 1)\n",
        "        self.assertEqual(peft_training_args.per_device_eval_batch_size, 1)\n",
        "\n",
        "# Run the tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestTrainingArguments))"
      ],
      "metadata": {
        "id": "DxDRkd-O22Tc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create the SFT Trainer.  The trainer class contains all the information necessary to train a model, including:\n",
        "* `model = peft_model`: Of course, the trainer requires the model for training.\n",
        "* `args = peft_training_args`: These are the training arguments we defined earlier.\n",
        "* `train_dataset = dataset_with_text['train']`: We will use the `train` portion of the dataset for fine tuning.\n",
        "* `eval_dataset = dataset_with_text['validation']`: We will use the `validation` portion of the dataset to periodically evaluate our training progress.\n",
        "* `dataset_text_field = text`: dataset_text_field that corresponds to text containing prompt and its response\n"
      ],
      "metadata": {
        "id": "zZtQilxt4blr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_training_args = create_peft_training_args()\n",
        "\n",
        "########################\n",
        "# START YOUR CODE HERE #\n",
        "########################\n",
        "# Replace None with your code\n",
        "\n",
        "peft_trainer = SFTTrainer(\n",
        "    None,\n",
        "    None,\n",
        "    None,\n",
        "    None,\n",
        "    None,\n",
        "    None,\n",
        ")\n",
        "\n",
        "######################\n",
        "# END YOUR CODE HERE #\n",
        "######################"
      ],
      "metadata": {
        "id": "wo4wNsS_22bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='6'></a>\n",
        "# Model Training\n",
        "\n",
        "These training arguments are verified to run properly on the free T4 GPUs available on Colab.  This training process will take under an hour.  If your notebook times out or you don't have enough time to train, we provide an option below to download the fine-tuned model from one of our course developers [here](https://huggingface.co/mrplants/arphiv).\n",
        "\n",
        "If you have some Colab credits available, this training process will take significantly less time using the V100 GPU (<15 minutes)."
      ],
      "metadata": {
        "id": "4jxH2G5J7QcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path=\"uplimit-project-3-phi-1.5\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "vYXvbkWkuFVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='7'></a>\n",
        "# Evaluation\n",
        "Now that the model training is complete, let's review the results using a couple samples from the test set."
      ],
      "metadata": {
        "id": "_zlF1kIhBDV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(source_content: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> str:\n",
        "    \"\"\"Generates a summary for the given academic content using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        source_content (str): The academic content to summarize.\n",
        "        model (AutoModelForCausalLM): The pretrained model used for text generation.\n",
        "        tokenizer (AutoTokenizer): The tokenizer used for encoding and decoding text.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated summary.\n",
        "    \"\"\"\n",
        "    prompt = prompt_template.format(source=source_content, bos_token = tokenizer.bos_token)\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda:0')\n",
        "\n",
        "    outputs = model.generate(input_ids=input_ids,\n",
        "                             max_new_tokens=100,\n",
        "                             do_sample=True,\n",
        "                             temperature=0.7,\n",
        "                             repetition_penalty=1.5,\n",
        "                             eos_token_id=tokenizer.eos_token_id,\n",
        "                             pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "    generated_ids = outputs[0][len(input_ids[0]):]\n",
        "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "def evaluate_model_summary(index: int, dataset: dict, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> None:\n",
        "    \"\"\"Evaluates and prints a model-generated summary against a human-created summary.\n",
        "\n",
        "    Args:\n",
        "        index (int): The index of the sample in the dataset to summarize.\n",
        "        dataset (dict): The dataset containing the academic content and human-created summaries.\n",
        "        model (AutoModelForCausalLM): The pretrained model used for text generation.\n",
        "        tokenizer (AutoTokenizer): The tokenizer used for encoding and decoding text.\n",
        "    \"\"\"\n",
        "    source_content = ' '.join(dataset['test'][index]['source'])\n",
        "    baseline_human_summary = dataset['test'][index]['target'][0]\n",
        "    peft_model_text_output = generate_summary(source_content, model, tokenizer)\n",
        "\n",
        "    print('-----------------------------')\n",
        "    print(f\"Summarize the following academic content.\\n\\n{source_content}\\n\\nSummary: \")\n",
        "    print('-----------------------------')\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
        "    print('-----------------------------')\n",
        "    print(f'PEFT MODEL SUMMARY:\\n{peft_model_text_output}')\n",
        "    print('-----------------------------')\n",
        "\n",
        "# Example usage\n",
        "evaluate_model_summary(0, dataset, peft_model, tokenizer)\n",
        "evaluate_model_summary(1, dataset, peft_model, tokenizer)"
      ],
      "metadata": {
        "id": "Ei3A_gwZDrKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're happy with the results, upload your LORA model to the HuggingFace Hub!"
      ],
      "metadata": {
        "id": "DpW-itdJF-RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "0EvGCrvSZT5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = peft_trainer.push_to_hub()\n",
        "print(f'Find your new model here:  {model_url}')"
      ],
      "metadata": {
        "id": "M10P0ErHaHx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name='8'></a>\n",
        "# Real World Application\n",
        "Now let's try out our model on some data from the wild! The following code retrieves the abstract from a paper in the arXiv database and summarizes it using our model.\n",
        "\n",
        "# RESTART THE KERNEL IF YOU DON'T HAVE ENOUGH GPU MEMORY\n",
        "\n",
        "## Step 1: Download the PEFT model from the hub and create a new model\n",
        "\n",
        "Since the model that is uploaded to the hub is a LoRA model, it is much smaller (~25MB for phi 1.5) than the original model. Go and check the size on HF hub.\n",
        "\n",
        "In order to use this model for inference, we need to add LoRA weights to original model's weights.\n",
        "\n",
        "Below code shows the steps to download a LoRA model from hub and create a model ready for inference."
      ],
      "metadata": {
        "id": "iouJ8fHz1jFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] datasets peft einops trl --quiet"
      ],
      "metadata": {
        "id": "DQR9sNTxk7C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "########################\n",
        "# START YOUR CODE HERE #\n",
        "########################\n",
        "# Replace None with your code\n",
        "\n",
        "peft_model_id = None\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "inference_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, device_map=\"auto\", trust_remote_code=True)\n",
        "inference_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)\n",
        "\n",
        "inference_tokenizer.pad_token_id = inference_tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "QpuYZTTy0bQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Download abstracts from arxiv and summarize them"
      ],
      "metadata": {
        "id": "WAywOPeTGMbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def fetch_arxiv_abstract_by_id(arxiv_id: str) -> str:\n",
        "    \"\"\"Fetches the abstract of a paper from the arXiv database using its ID.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id (str): The arXiv identifier for the paper.\n",
        "\n",
        "    Returns:\n",
        "        str: The abstract of the paper.\n",
        "    \"\"\"\n",
        "    # Construct the API URL for the arXiv paper\n",
        "    url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'\n",
        "\n",
        "    try:\n",
        "        # Fetch the XML data\n",
        "        response = urllib.request.urlopen(url)\n",
        "        xml_data = response.read().decode('utf-8')\n",
        "\n",
        "        # Parse the XML data\n",
        "        root = ET.fromstring(xml_data)\n",
        "\n",
        "        # Find the <summary> element which contains the abstract\n",
        "        for entry in root.find('{http://www.w3.org/2005/Atom}entry'):\n",
        "            if entry.tag == '{http://www.w3.org/2005/Atom}summary':\n",
        "                return entry.text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "    return \"Abstract not found\"\n",
        "\n",
        "arxiv_id = \"2103.00020\"  # Replace with the arXiv ID of the paper you're interested in\n",
        "abstract = fetch_arxiv_abstract_by_id(arxiv_id)\n",
        "print()\n",
        "print(f\"Abstract for paper {arxiv_id}:\\n{abstract}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "HJIIeQPcBxPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3\n",
        "\n",
        "Create a prompt template and create an abstract"
      ],
      "metadata": {
        "id": "qJ-sqRVknFjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Summarize the following academic content.\\n\n",
        "{source}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "prompt = prompt_template.format(source=abstract)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "kDem5z1-k0Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Use the model to summarize the abstract"
      ],
      "metadata": {
        "id": "aCTUeDtFnMqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = inference_tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda:0')\n",
        "\n",
        "outputs = inference_model.generate(input_ids=input_ids,\n",
        "                        max_new_tokens=50,\n",
        "                          do_sample=True,\n",
        "                          temperature=0.7,\n",
        "                          repetition_penalty=1.5,\n",
        "                          eos_token_id=inference_tokenizer.eos_token_id,\n",
        "                          pad_token_id=inference_tokenizer.pad_token_id)\n",
        "\n",
        "generated_ids = outputs[0][len(input_ids[0]):]\n",
        "summary = inference_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "print(\"TLDR target: \\n%s\"%summary)"
      ],
      "metadata": {
        "id": "XvNud8h7lIzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='9'></a>\n",
        "# Conclusion\n",
        "\n",
        "This project provided a comprehensive exploration into the realm of machine learning, specifically focusing on the application of Parameter-Efficient Fine-Tuning (PEFT) and Low Rank Adaptation (LORA) techniques. Utilizing the phi-1.5 model, we successfully fine-tuned it to generate concise and coherent summaries of scientific papers using the SciTLDR dataset from the Allen Institute for AI.\n",
        "\n",
        "Through the course of the project, we delved into various aspects such as data preprocessing and model trainingmetrics. Additionally, we explored ways to fetch real-world data from the arXiv database for practical testing. The project not only served as a hands-on experience in applying advanced NLP techniques but also laid the foundation for further research and development in automating the summarization of academic content."
      ],
      "metadata": {
        "id": "CUmNwgLiKiEX"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}